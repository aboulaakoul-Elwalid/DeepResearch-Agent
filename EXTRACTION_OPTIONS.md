# Shamela Book Extraction - Three Approaches Comparison

## The Problem

You want to extract **63 specific books** from a massive Hugging Face dataset containing **8,492+ books**.

**Issue**: Downloading the entire text dataset causes **OOM (Out of Memory)** errors.

---

## Solution Options

### âœ… Option 1: Smart Streaming Approach (RECOMMENDED)

**File**: `extract_books_smart.py`

**How it works**:
1. Download metadata first (small, ~8MB) âœ“ Already done!
2. Identify the 63 book IDs from metadata
3. **Stream** the text dataset (not loading everything into RAM)
4. Save each matching book immediately
5. Stop early once all 63 books are found

**Advantages**:
- âœ… No OOM issues (processes one book at a time)
- âœ… Can resume if interrupted
- âœ… Stops early when all books are found
- âœ… Progress bar shows real-time status
- âœ… Works reliably even on low-memory systems

**Disadvantages**:
- â±ï¸ May need to scan many books to find all 63
- ğŸŒ Requires stable internet (but can resume)

**Usage**:
```bash
python3 extract_books_smart.py
```

**Memory Usage**: ~500MB max (constant, regardless of dataset size)

---

### â­ Option 2: Direct Parquet Download (FASTEST)

**Manual approach using HF's parquet files**

**How it works**:
1. Download metadata (small parquet file)
2. Identify the 63 book row indices
3. Download text dataset's parquet file directly
4. Use pandas to filter only the 63 rows

**Advantages**:
- âœ… Very fast (direct file download)
- âœ… No streaming needed
- âœ… Uses efficient parquet format
- âœ… Can filter in pandas easily

**Disadvantages**:
- âš ï¸ Still downloads the full text parquet (but it's compressed)
- âš ï¸ Need ~2-5GB disk space temporarily
- âš ï¸ Requires finding the parquet URL from HF

**Usage**:
```python
import pandas as pd
import requests

# Get parquet URL from HF dataset page
metadata_url = "https://huggingface.co/datasets/MoMonir/Shamela_Books_info/resolve/main/data/train-00000-of-00001.parquet"
text_url = "https://huggingface.co/datasets/MoMonir/shamela_books_text_full/resolve/main/data/train-XXXXX.parquet"

# Download and filter
metadata_df = pd.read_parquet(metadata_url)
text_df = pd.read_parquet(text_url)

# Filter to your 63 books
filtered = text_df[text_df['id'].isin(your_book_ids)]
```

---

### ğŸ”§ Option 3: HF Dataset Viewer API (EXPERIMENTAL)

**File**: `extract_via_api.py` (incomplete)

**How it works**:
1. Use Hugging Face's Dataset Viewer API
2. Query specific rows without downloading
3. Fetch only the 63 books via HTTP requests

**Advantages**:
- âœ… Zero full download
- âœ… Minimal bandwidth usage
- âœ… Fast for small selections

**Disadvantages**:
- âš ï¸ API may have rate limits
- âš ï¸ Not officially documented for all datasets
- âš ï¸ May not work for all dataset configurations
- âš ï¸ Requires knowing exact row numbers beforehand

**Status**: Experimental, may not work reliably

---

## ğŸ“Š Comparison Table

| Feature | Streaming (Option 1) | Direct Parquet (Option 2) | API (Option 3) |
|---------|---------------------|--------------------------|----------------|
| Memory Usage | âœ… Low (~500MB) | âš ï¸ Medium (~2-5GB) | âœ… Low |
| Speed | â±ï¸ Medium-Slow | âœ… Fast | âœ… Very Fast |
| Reliability | âœ… High | âœ… High | âš ï¸ Medium |
| Network Usage | â±ï¸ Continuous | ğŸ“¥ One big download | âœ… Minimal |
| Resumable | âœ… Yes | âš ï¸ Partial | âŒ No |
| Setup Complexity | âœ… Easy | âš ï¸ Manual URLs | âš ï¸ Complex |

---

## ğŸ¯ Our Recommendation

**Use Option 1: Smart Streaming** (`extract_books_smart.py`)

### Why?
1. **Already set up** - Script is ready to run
2. **Guaranteed to work** - No OOM issues
3. **Progress tracking** - See real-time status
4. **Safe** - Saves books as they're found
5. **Stops early** - Won't scan entire dataset unnecessarily

### Run it now:
```bash
cd /home/elwalid/projects/parallax_project
python3 extract_books_smart.py
```

---

## ğŸ”„ Workflow: Separating Metadata & Text

You asked about separating metadata from text - **we're already doing this!**

### Current Approach (Smart):

```
Step 1: Download Metadata Only
â”œâ”€ Dataset: MoMonir/Shamela_Books_info
â”œâ”€ Size: ~8MB (small!)
â”œâ”€ Contains: Book titles, authors, IDs
â””â”€ Cache: output/metadata/shamela_info.parquet

Step 2: Identify Target Books
â”œâ”€ Search metadata for your 63 books
â”œâ”€ Extract book IDs and row indices
â””â”€ Save: output/metadata/matched_books.json

Step 3: Stream Text Dataset
â”œâ”€ Dataset: MoMonir/shamela_books_text_full
â”œâ”€ Method: Streaming (not full download)
â”œâ”€ Match: Check each book's ID against our 63 targets
â”œâ”€ Action: Save immediately when found
â””â”€ Output: output/books/01_book_title.md

Step 4: Create Master CSV
â”œâ”€ Combine metadata + extraction status
â””â”€ Save: output/selected_books.csv
```

### Why This Works:
- âœ… **Metadata is small** - Downloads fast, cached locally
- âœ… **Text is streamed** - No OOM, processed incrementally
- âœ… **Separation of concerns** - Identify first, download second
- âœ… **Efficient** - Only downloads what we need (in streaming fashion)

---

## ğŸš€ Quick Start (Right Now)

```bash
# Make sure you're in the project directory
cd /home/elwalid/projects/parallax_project

# Run the smart streaming extractor
python3 extract_books_smart.py

# It will:
# 1. Load cached metadata (already downloaded)
# 2. Identify your 63 books
# 3. Stream the text dataset
# 4. Save books as they're found
# 5. Show progress bar
# 6. Stop early when all 63 are found
```

---

## ğŸ“ Expected Output

```
parallax_project/
â””â”€â”€ output/
    â”œâ”€â”€ selected_books.csv              # Master metadata file
    â”œâ”€â”€ metadata/
    â”‚   â”œâ”€â”€ shamela_info.parquet       # Cached metadata (8,492 books)
    â”‚   â””â”€â”€ matched_books.json         # Your 63 book IDs
    â””â”€â”€ books/                         # Individual book files
        â”œâ”€â”€ 01_ØªÙØ³ÙŠØ±_Ø§Ù„Ø·Ø¨Ø±ÙŠ.md
        â”œâ”€â”€ 02_ØµØ­ÙŠØ­_Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ.md
        â”œâ”€â”€ 03_ØµØ­ÙŠØ­_Ù…Ø³Ù„Ù….md
        â””â”€â”€ ... (63 files total)
```

---

## ğŸ’¡ Key Insight: Chunking vs Streaming

You asked about "downloading in chunks" - here's the clarification:

### Chunks (Traditional)
```python
# Downloads in batches
for i in range(0, total, chunk_size):
    chunk = download_data(start=i, end=i+chunk_size)
    process(chunk)
    # Still loads chunks into memory
```

### Streaming (Better)
```python
# Processes one item at a time
for item in stream_data():
    process(item)
    # Only one item in memory at a time
```

**Our approach uses streaming**, which is even better than chunking because memory usage stays constant (one book at a time) regardless of dataset size.

---

## ğŸ“ Standard Practice Summary

When working with HF datasets where you need a **small subset** from a **large dataset**:

1. âœ… **Download metadata separately** (always small)
2. âœ… **Identify target items** from metadata
3. âœ… **Use streaming** for large text data
4. âœ… **Save incrementally** (don't wait for everything)
5. âœ… **Cache metadata** (avoid re-downloading)
6. âœ… **Use parquet** for storage (efficient)

This is exactly what `extract_books_smart.py` does!

---

## âš¡ Ready to Run?

```bash
python3 extract_books_smart.py
```

Estimated time: 30-60 minutes (depends on network and how early we find all 63 books)

The script will show:
- âœ… Which books are found
- â±ï¸ Progress bar with stats
- ğŸ’¾ Real-time saving (won't lose progress)
- ğŸ“Š Final summary
