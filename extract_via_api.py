#!/usr/bin/env python3
"""
API-Based Shamela Book Extractor - Zero Download Approach

This script uses Hugging Face's Dataset Viewer API to query specific rows
without downloading the entire dataset. Perfect for extracting only 63 books
from a massive dataset.

Benefits:
- No OOM issues
- No large downloads
- Fast and efficient
- Uses HF's parquet files directly

Usage:
    python extract_via_api.py
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd
import requests
from tqdm import tqdm

# List of 63 books to extract
SELECTED_BOOKS = [
    # 1. Tafsir (Quranic Exegesis) - 14 books
    "ØªÙØ³ÙŠØ± Ø§Ù„Ø·Ø¨Ø±ÙŠ",
    "Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù† Ø¹Ù† ØªØ£ÙˆÙŠÙ„ Ø¢ÙŠ Ø§Ù„Ù‚Ø±Ø¢Ù†",
    "ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„Ø¹Ø¸ÙŠÙ…",
    "Ø§Ù„Ø¬Ø§Ù…Ø¹ Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø±Ø¢Ù†",
    "Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØºÙŠØ¨",
    "Ø§Ù„ØªÙØ³ÙŠØ± Ø§Ù„ÙƒØ¨ÙŠØ±",
    "Ø§Ù„ÙƒØ´Ø§Ù Ø¹Ù† Ø­Ù‚Ø§Ø¦Ù‚ ØºÙˆØ§Ù…Ø¶ Ø§Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆØ¹ÙŠÙˆÙ† Ø§Ù„Ø£Ù‚Ø§ÙˆÙŠÙ„ ÙÙŠ ÙˆØ¬ÙˆÙ‡ Ø§Ù„ØªØ£ÙˆÙŠÙ„",
    "Ø£Ù†ÙˆØ§Ø± Ø§Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆØ£Ø³Ø±Ø§Ø± Ø§Ù„ØªØ£ÙˆÙŠÙ„",
    "Ù…Ø¹Ø§Ù„Ù… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ ÙÙŠ ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù†",
    "ØªÙØ³ÙŠØ± Ø§Ù„Ø¨ØºÙˆÙŠ",
    "Ø±ÙˆØ­ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ ÙÙŠ ØªÙØ³ÙŠØ± Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„Ø¹Ø¸ÙŠÙ… ÙˆØ§Ù„Ø³Ø¨Ø¹ Ø§Ù„Ù…Ø«Ø§Ù†ÙŠ",
    "ØªÙØ³ÙŠØ± Ø§Ù„Ø¬Ù„Ø§Ù„ÙŠÙ†",
    "Ø£Ø¶ÙˆØ§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù† ÙÙŠ Ø¥ÙŠØ¶Ø§Ø­ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø¨Ø§Ù„Ù‚Ø±Ø¢Ù†",
    "Ø§Ù„Ù…Ø­Ø±Ø± Ø§Ù„ÙˆØ¬ÙŠØ² ÙÙŠ ØªÙØ³ÙŠØ± Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø¹Ø²ÙŠØ²",
    "Ø§Ù„Ø¯Ø± Ø§Ù„Ù…Ù†Ø«ÙˆØ±",
    "Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø±Ø¢Ù†",
    "Ø²Ø§Ø¯ Ø§Ù„Ù…Ø³ÙŠØ± ÙÙŠ Ø¹Ù„Ù… Ø§Ù„ØªÙØ³ÙŠØ±",
    # 2. Hadith (Prophetic Traditions) - 21 books
    "ØµØ­ÙŠØ­ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ",
    "ØµØ­ÙŠØ­ Ù…Ø³Ù„Ù…",
    "Ø³Ù†Ù† Ø£Ø¨ÙŠ Ø¯Ø§ÙˆØ¯",
    "Ø³Ù†Ù† Ø§Ù„ØªØ±Ù…Ø°ÙŠ",
    "Ø³Ù†Ù† Ø§Ù„Ù†Ø³Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø¬ØªØ¨Ù‰",
    "Ø³Ù†Ù† Ø§Ø¨Ù† Ù…Ø§Ø¬Ù‡",
    "Ø§Ù„Ù…ÙˆØ·Ø£",
    "Ø§Ù„Ù…Ø³ØªØ¯Ø±Ùƒ Ø¹Ù„Ù‰ Ø§Ù„ØµØ­ÙŠØ­ÙŠÙ†",
    "ØµØ­ÙŠØ­ Ø§Ø¨Ù† Ø®Ø²ÙŠÙ…Ø©",
    "ØµØ­ÙŠØ­ Ø§Ø¨Ù† Ø­Ø¨Ø§Ù† Ø¨ØªØ±ØªÙŠØ¨ Ø§Ø¨Ù† Ø¨Ù„Ø¨Ø§Ù†",
    "Ø§Ù„Ù…Ù†Ù‡Ø§Ø¬ Ø´Ø±Ø­ ØµØ­ÙŠØ­ Ù…Ø³Ù„Ù… Ø¨Ù† Ø§Ù„Ø­Ø¬Ø§Ø¬",
    "ÙØªØ­ Ø§Ù„Ø¨Ø§Ø±ÙŠ",
    "Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¹Ù„Ù‰ ÙØªØ­ Ø§Ù„Ø¨Ø§Ø±ÙŠ",
    "ØªØ­ÙØ© Ø§Ù„Ø£Ø­ÙˆØ°ÙŠ Ø¨Ø´Ø±Ø­ Ø¬Ø§Ù…Ø¹ Ø§Ù„ØªØ±Ù…Ø°ÙŠ",
    "Ø¹ÙˆÙ† Ø§Ù„Ù…Ø¹Ø¨ÙˆØ¯",
    "Ø³Ù†Ù† Ø£Ø¨ÙŠ Ø¯Ø§ÙˆØ¯ Ù…Ø¹ Ø´Ø±Ø­Ù‡ Ø¹ÙˆÙ† Ø§Ù„Ù…Ø¹Ø¨ÙˆØ¯",
    "Ù†ÙŠÙ„ Ø§Ù„Ø£ÙˆØ·Ø§Ø±",
    "Ø³Ø¨Ù„ Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù…ÙˆØµÙ„Ø© Ø¥Ù„Ù‰ Ø¨Ù„ÙˆØº Ø§Ù„Ù…Ø±Ø§Ù…",
    "Ø±ÙŠØ§Ø¶ Ø§Ù„ØµØ§Ù„Ø­ÙŠÙ†",
    "Ø§Ù„Ø£Ø±Ø¨Ø¹ÙˆÙ† Ø§Ù„Ù†ÙˆÙˆÙŠØ©",
    "Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„ÙƒØ¨Ø±Ù‰",
    "Ù…Ø´ÙƒØ§Ø© Ø§Ù„Ù…ØµØ§Ø¨ÙŠØ­",
    "Ø§Ù„ØªØ±ØºÙŠØ¨ ÙˆØ§Ù„ØªØ±Ù‡ÙŠØ¨",
    # 3. Fiqh & Usul (Jurisprudence & Principles) - 17 books
    "Ø§Ù„Ø£Ù…",
    "Ø§Ù„Ø±Ø³Ø§Ù„Ø©",
    "Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©",
    "Ø§Ù„Ù…Ø¨Ø³ÙˆØ·",
    "Ø¨Ø¯Ø§Ø¦Ø¹ Ø§Ù„ØµÙ†Ø§Ø¦Ø¹ ÙÙŠ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø´Ø±Ø§Ø¦Ø¹",
    "Ø§Ù„Ù…ØºÙ†ÙŠ",
    "Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø´Ø±Ø­ Ø§Ù„Ù…Ù‡Ø°Ø¨",
    "Ø±ÙˆØ¶Ø© Ø§Ù„Ø·Ø§Ù„Ø¨ÙŠÙ† ÙˆØ¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙØªÙŠÙ†",
    "Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ø¬ØªÙ‡Ø¯ ÙˆÙ†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù‚ØªØµØ¯",
    "ÙƒØ´Ø§Ù Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¹Ù† Ø§Ù„Ø¥Ù‚Ù†Ø§Ø¹",
    "Ø§Ù„Ø°Ø®ÙŠØ±Ø©",
    "Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø§Øª",
    "Ø¥Ø¹Ù„Ø§Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ÙŠÙ† Ø¹Ù† Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†",
    "Ø£ØµÙˆÙ„ Ø§Ù„ÙÙ‚Ù‡",
    "Ø§Ù„Ø¨Ø±Ù‡Ø§Ù† ÙÙŠ Ø£ØµÙˆÙ„ Ø§Ù„ÙÙ‚Ù‡",
    "Ø§Ù„Ù…Ø³ØªØµÙÙ‰",
    "Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ø­ÙƒÙ…ÙŠØ©",
    # 4. Tarikh & Rijal (History & Biography) - 11 books
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø·Ø¨Ø±ÙŠ",
    "ØªØ§Ø±ÙŠØ® Ø§Ù„Ø±Ø³Ù„ ÙˆØ§Ù„Ù…Ù„ÙˆÙƒ",
    "Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®",
    "Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©",
    "Ø³ÙŠØ± Ø£Ø¹Ù„Ø§Ù… Ø§Ù„Ù†Ø¨Ù„Ø§Ø¡",
    "ØªØ§Ø±ÙŠØ® Ø¨ØºØ¯Ø§Ø¯",
    "Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ÙƒØ¨Ø±Ù‰",
    "Ø§Ù„Ø¥ØµØ§Ø¨Ø© ÙÙŠ ØªÙ…ÙŠÙŠØ² Ø§Ù„ØµØ­Ø§Ø¨Ø©",
    "Ø£Ø³Ø¯ Ø§Ù„ØºØ§Ø¨Ø© ÙÙŠ Ù…Ø¹Ø±ÙØ© Ø§Ù„ØµØ­Ø§Ø¨Ø©",
    "ØªÙ‡Ø°ÙŠØ¨ Ø§Ù„ØªÙ‡Ø°ÙŠØ¨",
    "ÙˆÙÙŠØ§Øª Ø§Ù„Ø£Ø¹ÙŠØ§Ù† ÙˆØ£Ù†Ø¨Ø§Ø¡ Ø£Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø²Ù…Ø§Ù†",
    "Ø§Ù„Ø±ÙˆØ¶ Ø§Ù„Ø£Ù†Ù ÙÙŠ Ø´Ø±Ø­ Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ù†Ø¨ÙˆÙŠØ©",
]

# HF API endpoints
HF_API_BASE = "https://datasets-server.huggingface.co"
HF_DATASETS_API = "https://huggingface.co/api/datasets"


def setup_directories():
    """Create output directories."""
    dirs = ["output", "output/books", "output/metadata"]
    for d in dirs:
        Path(d).mkdir(parents=True, exist_ok=True)


def fetch_metadata_via_api(dataset_name: str = "MoMonir/Shamela_Books_info") -> pd.DataFrame:
    """
    Fetch metadata using HF's parquet files API.
    This is much faster than downloading the entire dataset.
    """
    print("\n" + "=" * 80)
    print("STEP 1: Fetching Metadata via API")
    print("=" * 80)

    cache_file = Path("output/metadata/shamela_info.parquet")

    if cache_file.exists():
        print(f"âœ“ Loading cached metadata from {cache_file}")
        return pd.read_parquet(cache_file)

    print(f"ðŸ“¥ Fetching metadata from {dataset_name}...")

    try:
        # Get parquet files info
        url = f"{HF_DATASETS_API}/{dataset_name}/parquet"
        response = requests.get(url)
        response.raise_for_status()

        parquet_info = response.json()

        # Download the parquet file directly
        if "train" in parquet_info:
            parquet_url = parquet_info["train"][0]["url"]
            print(f"ðŸ“¥ Downloading parquet: {parquet_url}")

            df = pd.read_parquet(parquet_url)
            df.to_parquet(cache_file, index=False)
            print(f"âœ“ Cached to {cache_file}")
            print(f"âœ“ Loaded {len(df)} metadata records")

            return df

    except Exception as e:
        print(f"âš ï¸  API method failed: {e}")
        print("ðŸ“¥ Falling back to datasets library...")

        from datasets import load_dataset

        info_dataset = load_dataset(dataset_name)
        df = pd.DataFrame(info_dataset["train"])
        df.to_parquet(cache_file, index=False)
        print(f"âœ“ Loaded {len(df)} metadata records")

        return df


def query_rows_api(
    dataset_name: str, offset: int = 0, length: int = 100
) -> Optional[Dict]:
    """
    Query specific rows from HF dataset using the viewer API.
    """
    url = f"{HF_API_BASE}/rows"
    params = {
        "dataset": dataset_name,
        "config": "default",
        "split": "train",
        "offset": offset,
        "length": length,
    }

    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"âš ï¸  API query failed at offset {offset}: {e}")
        return None


def search_api(dataset_name: str, query: str) -> Optional[Dict]:
    """
    Search dataset using HF's search API (if available).
    """
    url = f"{HF_API_BASE}/search"
    params = {"dataset": dataset_name, "config": "default", "split": "train", "query": query}

    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        return response.json()
    except:
        return None


def identify_books_from_metadata(
    metadata_df: pd.DataFrame, selected_books: List[str]
) -> Dict:
    """
    Identify target books from metadata.
    Returns mapping of indices to book info.
    """
    print("\n" + "=" * 80)
    print("STEP 2: Identifying Target Books")
    print("=" * 80)

    # Find title column
    title_col = None
    for col in metadata_df.columns:
        if any(kw in col.lower() for kw in ["title", "name", "book", "Ø¹Ù†ÙˆØ§Ù†"]):
            title_col = col
            break

    if not title_col:
        title_col = metadata_df.columns[0]

    print(f"ðŸ” Using column '{title_col}' for titles")

    matched_books = {}
    not_found = []

    for book_title in selected_books:
        # Try exact match
        mask = metadata_df[title_col].astype(str) == book_title
        matches = metadata_df[mask]

        # Try partial match
        if len(matches) == 0:
            mask = (
                metadata_df[title_col]
                .astype(str)
                .str.contains(book_title, na=False, regex=False)
            )
            matches = metadata_df[mask]

        if len(matches) > 0:
            idx = matches.index[0]
            row = matches.iloc[0]

            matched_books[idx] = {
                "index": idx,
                "title": row[title_col],
                "metadata": row.to_dict(),
            }
            print(f"âœ“ Found: {book_title} (row index: {idx})")
        else:
            not_found.append(book_title)
            print(f"âœ— Not found: {book_title}")

    print(f"\nðŸ“Š Summary:")
    print(f"  Requested: {len(selected_books)}")
    print(f"  Found: {len(matched_books)}")
    print(f"  Missing: {len(not_found)}")

    if not_found and len(not_found) <= 10:
        print(f"\nâš ï¸  Missing books:")
        for title in not_found:
            print(f"    - {title}")

    # Save matched info
    with
